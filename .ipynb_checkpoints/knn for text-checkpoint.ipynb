{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/giorgiomondauto/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giorgiomondauto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giorgiomondauto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/giorgiomondauto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'rec.motorcycles', 'sci.electronics', 'sci.med']\n",
      "From: kreyling@lds.loral.com (Ed Kreyling 6966)\n",
      "Subject: Sun-os and 8bit ASCII graphics\n",
      "Organization: Loral Data Systems\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "rec.motorcycles\n",
      "comp.graphics\n",
      "sci.med\n",
      "sci.electronics\n",
      "sci.electronics\n",
      "comp.graphics\n",
      "rec.motorcycles\n",
      "sci.electronics\n"
     ]
    }
   ],
   "source": [
    "categories = ['rec.motorcycles', 'sci.electronics',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "# sklearn provides us with subset data for training and testing\n",
    "train_data = fetch_20newsgroups(subset='train',\n",
    "                                categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(train_data.target_names)\n",
    "\n",
    "print(\"\\n\".join(train_data.data[0].split(\"\\n\")[:3]))\n",
    "print(train_data.target_names[train_data.target[0]])\n",
    "\n",
    "# Let's look at categories of our first ten training data\n",
    "for t in train_data.target[:10]:\n",
    "    print(train_data.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ragee@vdoe386.vak12ed.edu (Randy Agee)\n",
      "Subject: Radar detector DETECTORS?\n",
      "Organization: Virginia's Public Education Network (Richmond)\n",
      "Lines: 27\n",
      "\n",
      "Several years back one of the radar detectors manufacturers, in\n",
      "defiance to Virginia's law against radar detectors, passed out\n",
      "thousands of fake cardboard radar detectors at truck stops near\n",
      "the Virginia State lines.  At that time there were no radar\n",
      "detector Detectors!  I am not sure of the impact but I would\n",
      "imagine that enforcement of the law by visually sighting a\n",
      "radar detector became difficult - if not impossible!\n",
      "\n",
      "As I said earlier, efforts to throw out or eliminate the VA law\n",
      "against radar detectors has been in vain.  In fact, effective\n",
      "Jan. 1, 1993, the fine for possession of a radar detector\n",
      "accessable to the driver of a vehicle in VA is now $250.00.  \n",
      "\n",
      "I have noted an interesting anomality with my Alinco DR-100 2\n",
      "meter ham transceiver.... It will make a *cheap* radar detector\n",
      "scream!  I am not sure of the range, but it is obvious by the\n",
      "brake lights that it can be at least 50 feet at 50 watts! :-)\n",
      "\n",
      "==============================================================================\n",
      "Randy T. Agee - WB4BZX           | At some point, you probably pondered The \n",
      "P.O. Box 2120 - 20th floor       | Meaning of Life, and you came up with a \n",
      "Virginia Department of Education | satisfactory answer, which has or has not\n",
      "Richmond, VA 23216-2120          | stood the test of time, or you shrugged\n",
      "Phone (804) 225-2669             | mightily, muttered \"Beats the heck out of\n",
      "ragee@vdoe386.vak12ed.edu        | me,\" and ordered a cheeseburger.\n",
      "=============================================================================\n",
      " \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-fb9b20e0fe3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(train_data.data[5])\n",
    "train_data.target_names[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary of features and transforms documents to feature vectors and convert our text documents to a\n",
    "# matrix of token counts (CountVectorizer)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data.data)\n",
    "\n",
    "# transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf = knn.fit(X_train_tfidf, train_data.target)\n",
    "\n",
    "# Input Data to predict their classes of the given categories\n",
    "docs_new = ['I have a Harley Davidson and Yamaha.', 'I have a GTX 1050 GPU']\n",
    "# building up feature vector of our input\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "# We call transform instead of fit_transform because it's already been fit\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I have a Harley Davidson and Yamaha.' => rec.motorcycles\n",
      "'I have a GTX 1050 GPU' => sci.med\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train_data.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got an accuracy of 82.67766497461929 % over the test data.\n"
     ]
    }
   ],
   "source": [
    "# We can use Pipeline to add vectorizer -> transformer -> classifier all in a one compound classifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', knn),\n",
    "])\n",
    "# Fitting our train data to the pipeline\n",
    "text_clf.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Test data \n",
    "test_data = fetch_20newsgroups(subset='test',\n",
    "                               categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = test_data.data\n",
    "# Predicting our test data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print('We got an accuracy of',np.mean(predicted == test_data.target)*100, '% over the test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = train_data.data[:2]\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       4, 1, 1, 1, 1, 1, 2, 1, 1, 5, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
       "       1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08195596, 0.05831234, 0.08195596, 0.08195596, 0.08195596,\n",
       "       0.16391192, 0.08195596, 0.08195596, 0.08195596, 0.23324936,\n",
       "       0.08195596, 0.2915617 , 0.08195596, 0.08195596, 0.32782383,\n",
       "       0.05831234, 0.08195596, 0.08195596, 0.08195596, 0.08195596,\n",
       "       0.05831234, 0.08195596, 0.05831234, 0.24586788, 0.24586788,\n",
       "       0.05831234, 0.08195596, 0.16391192, 0.05831234, 0.08195596,\n",
       "       0.08195596, 0.16391192, 0.05831234, 0.05831234, 0.05831234,\n",
       "       0.08195596, 0.05831234, 0.08195596, 0.08195596, 0.16391192,\n",
       "       0.08195596, 0.05831234, 0.08195596, 0.08195596, 0.08195596,\n",
       "       0.05831234, 0.08195596, 0.08195596, 0.08195596, 0.08195596,\n",
       "       0.05831234, 0.24586788, 0.08195596, 0.24586788, 0.08195596,\n",
       "       0.08195596, 0.05831234, 0.24586788, 0.08195596, 0.05831234,\n",
       "       0.08195596, 0.11662468, 0.05831234, 0.08195596, 0.08195596,\n",
       "       0.08195596, 0.08195596, 0.08195596, 0.08195596, 0.08195596,\n",
       "       0.09698017, 0.06900219, 0.09698017, 0.09698017, 0.09698017,\n",
       "       0.09698017, 0.20700657, 0.29094051, 0.13800438, 0.09698017,\n",
       "       0.09698017, 0.09698017, 0.06900219, 0.09698017, 0.19396034,\n",
       "       0.09698017, 0.09698017, 0.09698017, 0.09698017, 0.09698017,\n",
       "       0.06900219, 0.09698017, 0.09698017, 0.19396034, 0.09698017,\n",
       "       0.06900219, 0.09698017, 0.09698017, 0.20700657, 0.09698017,\n",
       "       0.09698017, 0.09698017, 0.19396034, 0.09698017, 0.06900219,\n",
       "       0.06900219, 0.09698017, 0.09698017, 0.09698017, 0.06900219,\n",
       "       0.20700657, 0.09698017, 0.09698017, 0.06900219, 0.09698017,\n",
       "       0.09698017, 0.19396034, 0.09698017, 0.13800438, 0.09698017,\n",
       "       0.19396034, 0.09698017, 0.09698017, 0.06900219, 0.09698017,\n",
       "       0.09698017, 0.09698017, 0.09698017, 0.06900219, 0.09698017,\n",
       "       0.19396034, 0.06900219, 0.09698017, 0.09698017, 0.06900219,\n",
       "       0.09698017, 0.06900219, 0.13800438, 0.09698017, 0.09698017,\n",
       "       0.09698017, 0.09698017, 0.09698017])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'rec.motorcycles']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target_names[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf = knn.fit(X_train_tfidf, train_data.target[:2])\n",
    "\n",
    "# Input Data to predict their classes of the given categories\n",
    "docs_new = ['I have a Harley Davidson and Yamaha.', 'I have a GTX 1050 GPU']\n",
    "# building up feature vector of our input\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "# We call transform instead of fit_transform because it's already been fit\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I have a Harley Davidson and Yamaha.' => comp.graphics\n",
      "'I have a GTX 1050 GPU' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train_data.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
